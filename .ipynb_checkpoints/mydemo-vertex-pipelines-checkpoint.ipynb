{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc09e5d-cd17-4b60-ae2f-800f1fb3a3a6",
   "metadata": {},
   "source": [
    "### The aim of this demo is to show the end to end ML workflow in a simple way in Vertex AI using some MLOps components such as Model Monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52959f85-a657-414d-9307-5c5be01f85ec",
   "metadata": {},
   "source": [
    "### Prepare environment, install libraries, set up environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01926a3a-3c55-4774-b653-804df1535d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing libraries\n"
     ]
    }
   ],
   "source": [
    "print(\"Installing libraries\")\n",
    "USER_FLAG = \"--user\"\n",
    "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-pipeline-components kfp\n",
    "! pip3 install {USER_FLAG} --quiet --upgrade google-cloud-aiplatform google-cloud-bigquery\n",
    "! pip3 install {USER_FLAG} --quiet db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18953f8f-5536-4eab-a1a6-b4a6a492e9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restart kernel after the installation of libraries\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d62267-6663-4e70-9849-2b3b02992db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  tadelle-372416\n"
     ]
    }
   ],
   "source": [
    "shell_output = !gcloud config list --format 'value(core.project)' \n",
    "PROJECT_ID = shell_output[0]\n",
    "print(\"Project ID: \", PROJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6828cc9-f6fb-47b2-8f16-dd014d33b1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230124125520\n"
     ]
    }
   ],
   "source": [
    "# timestamp - refresh when resubmitting pipeline runs\n",
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2790b1fe-2929-45aa-b0e3-997222dc3da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://tadelle-bucket/finance/\n",
      "                                 gs://tadelle-bucket/hcls/\n",
      "                                 gs://tadelle-bucket/telco/\n",
      "tadelle-bucket in us-central1\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME = \"tadelle-bucket\" \n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "! gsutil ls -al $BUCKET_URI\n",
    "REGION = \"us-central1\"\n",
    "print(BUCKET_NAME+\" in \"+REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f282757-52a1-46ab-9f7b-0edf930cf0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertex-pipelines-sa@tadelle-372416.iam.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = \"vertex-pipelines-sa@\" + PROJECT_ID + \".iam.gserviceaccount.com\"\n",
    "print(SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abef79d-58a8-4343-ad55-fb8206f68a47",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b85f91-444f-4d9d-bfb1-5773e19fc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import NamedTuple\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform as vertex\n",
    "from google.cloud import bigquery\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_pipeline_components\n",
    "from google_cloud_pipeline_components.experimental import bigquery as bq_components\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Input, Metrics, Output, component, ClassificationMetrics)\n",
    "\n",
    "#import kfp\n",
    "#from kfp.v2 import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a85d98e-ae08-47a8-9adf-a02bb394e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON_PKG_PATH = \"churn-pipeline.json\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root\"\n",
    "DATA_FOLDER = f\"{BUCKET_NAME}/data\"\n",
    "\n",
    "BQ_DATASET = \"demo\"  \n",
    "BQ_DATASET_TABLE = \"churn\"\n",
    "BQ_LOCATION = \"us-central1\"  \n",
    "BQ_LOCATION = BQ_LOCATION.upper()\n",
    "BQML_EXPORT_LOCATION = f\"gs://{BUCKET_NAME}/artifacts/bqml\"\n",
    "\n",
    "DISPLAY_NAME = \"churn\"\n",
    "ENDPOINT_DISPLAY_NAME = f\"{DISPLAY_NAME}_endpoint\"\n",
    "\n",
    "image_prefix = REGION.split(\"-\")[0]\n",
    "BQML_SERVING_CONTAINER_IMAGE_URI = (    \n",
    "    f\"{image_prefix}-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122e9b7-ebd2-463a-bada-d890fcc4b21e",
   "metadata": {},
   "source": [
    "### Pipeline components\n",
    "### Split the data for Train/Test using BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b7020d-ad9b-4db4-be50-bd2efc1a4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-bigquery\",\"pandas\",\"pyarrow\",\"fsspec\"],\n",
    ")  # pandas, pyarrow and fsspec required to export bq data to csv\n",
    "\n",
    "def split_datasets(\n",
    "    project: str,\n",
    "    bq_location: str,\n",
    "    bq_dataset: str,\n",
    "    bq_dataset_table: str\n",
    ") -> NamedTuple( \"Outputs\", [(\"dataset_uri\", str), (\"dataset_bq_uri\", str)]):\n",
    "\n",
    "    from collections import namedtuple\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project, location=bq_location)\n",
    "\n",
    "    def split_dataset():\n",
    "        \n",
    "        train_dataset_table = f\"{project}.{bq_dataset}.{bq_dataset_table}_train\"\n",
    "        test_dataset_table = f\"{project}.{bq_dataset}.{bq_dataset_table}_test\"\n",
    "        \n",
    "        split_query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{test_dataset_table}` AS\n",
    "        SELECT *\n",
    "        FROM `{project}.{bq_dataset}.{bq_dataset_table}`\n",
    "        where RAND() <= 20/100;\n",
    "        \n",
    "        CREATE OR REPLACE TABLE `{train_dataset_table}` AS\n",
    "        SELECT *\n",
    "        FROM\n",
    "         (\n",
    "         SELECT *\n",
    "         FROM `{project}.{bq_dataset}.{bq_dataset_table}` \n",
    "         EXCEPT distinct select * from `{test_dataset_table}`\n",
    "         );\n",
    "        \"\"\"\n",
    "        print(\"Splitting the dataset\")\n",
    "        \n",
    "        query_job = client.query(split_query)  \n",
    "        query_job.result()\n",
    "        \n",
    "        return (train_dataset_table, test_dataset_table) \n",
    "\n",
    "    dataset_uri = split_dataset()\n",
    "    \n",
    "    print(\"splitting dataset as training and test...\")\n",
    "    dataset_bq_uri = \"bq://\" + dataset_uri\n",
    "\n",
    "    print(f\"dataset: {dataset_uri}\")\n",
    "\n",
    "    result_tuple = namedtuple(\n",
    "        \"Outputs\",\n",
    "        [\"dataset_uri\", \"dataset_bq_uri\"],\n",
    "    )\n",
    "    return result_tuple(\n",
    "        dataset_uri=str(dataset_uri),\n",
    "        dataset_bq_uri=str(dataset_bq_uri),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "107c65cf-31a3-47f4-ab7d-5f8312bf743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"project\": PROJECT_ID, \n",
    "    \"location\": BQ_LOCATION,\n",
    "    \"bq_dataset\": BQ_DATASET,\n",
    "    \"bq_dataset_table\": BQ_DATASET_TABLE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04169ceb-8799-4528-89e0-3c37b6c47d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=DISPLAY_NAME, description=\"Simple pipeline\")\n",
    "def train_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    bq_dataset: str,\n",
    "    bq_dataset_table: str\n",
    "):\n",
    "\n",
    "    # Splits the BQ dataset using a custom component.\n",
    "    split_datasets_op = split_datasets(project=project, bq_location=location, bq_dataset=bq_dataset, bq_dataset_table=bq_dataset_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e1fdc8b-fe79-469b-a2d7-14c5c9084962",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=train_pipeline,\n",
    "    package_path=PIPELINE_JSON_PKG_PATH,\n",
    ")\n",
    "\n",
    "vertex.init(project=PROJECT_ID, location=BQ_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ce8e88-d44d-4efe-b11f-3aef5a7bc9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "ename": "PermissionDenied",
     "evalue": "403 Permission 'aiplatform.pipelineJobs.create' denied on resource '//aiplatform.googleapis.com/projects/tadelle-372416/locations/US-CENTRAL1' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\ndomain: \"aiplatform.googleapis.com\"\nmetadata {\n  key: \"permission\"\n  value: \"aiplatform.pipelineJobs.create\"\n}\nmetadata {\n  key: \"resource\"\n  value: \"projects/tadelle-372416/locations/US-CENTRAL1\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Permission 'aiplatform.pipelineJobs.create' denied on resource '//aiplatform.googleapis.com/projects/tadelle-372416/locations/US-CENTRAL1' (or it may not exist).\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:64.233.183.95:443 {grpc_message:\"Permission \\'aiplatform.pipelineJobs.create\\' denied on resource \\'//aiplatform.googleapis.com/projects/tadelle-372416/locations/US-CENTRAL1\\' (or it may not exist).\", grpc_status:7, created_time:\"2023-01-24T13:52:57.744557759+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPermissionDenied\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/835391260.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, service_account, network, create_request_timeout, experiment)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpipeline_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mpipeline_job_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/pipeline_service/client.py\u001b[0m in \u001b[0;36mcreate_pipeline_job\u001b[0;34m(self, request, parent, pipeline_job, pipeline_job_id, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m         )\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDenied\u001b[0m: 403 Permission 'aiplatform.pipelineJobs.create' denied on resource '//aiplatform.googleapis.com/projects/tadelle-372416/locations/US-CENTRAL1' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\ndomain: \"aiplatform.googleapis.com\"\nmetadata {\n  key: \"permission\"\n  value: \"aiplatform.pipelineJobs.create\"\n}\nmetadata {\n  key: \"resource\"\n  value: \"projects/tadelle-372416/locations/US-CENTRAL1\"\n}\n]"
     ]
    }
   ],
   "source": [
    "pipeline_job = vertex.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=PIPELINE_JSON_PKG_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    job_id=\"churn-pipeline-{0}\".format(TIMESTAMP),\n",
    "    parameter_values=pipeline_params,\n",
    "    enable_caching=True\n",
    ")\n",
    "\n",
    "response = pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf40c45-10c1-4935-87a5-c3e88488d842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "local-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
